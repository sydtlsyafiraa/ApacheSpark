{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e532adbd-97b9-46d9-b61a-c12437cc3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c220d0-0080-4784-b0bc-de5af8d24be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder.master(\"local\").appName(\"Test Spark\").getOrCreate()\n",
    "sc = spark.sparkContext   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d405d0d-d7f0-45aa-acfa-4860509ae61d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-VDOMLP1N.realtek:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2782f4f0d00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb1a732-7772-48e2-bace-51cb890d8b42",
   "metadata": {},
   "source": [
    "### Make the data set folder accessible\n",
    "\n",
    "In the following cells, we are going to load a file called `location_temp.csv`, which is a time-series file which contains loacations of sensors and the temperatures taken at particular periods of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8c47627-1f09-40c3-a10c-f65ec601f8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Syaidatul Syafira\\\\OneDrive - studentupmedumy.onmicrosoft.com\\\\Desktop\\\\DA\\\\Big Data Analytics with Apache Spark\\\\Apache Spark SC\\\\data'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "MAIN_DIRECTORY = os.getcwd()\n",
    "MAIN_DIRECTORY\n",
    "\n",
    "# os is operating system, cwd (current working directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab669ef8-e53b-4fdf-a363-79717e5d3cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = MAIN_DIRECTORY + \"/location_temp.csv\"\n",
    "\n",
    "# file_path is new variable, concate data location_temp dari folder data\n",
    "# + ........... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf678c2-8f5c-4b5d-930b-ad3d1b800cbb",
   "metadata": {},
   "source": [
    "## Get Started with Spark DataFrames\n",
    "To create a Spark DataFrame by loading a csv file, we can use `spark.read` function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7e7551b-bb5a-4e57-9b84-810a390bbb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.format(\"csv\").option(\"header\",\"true\").load(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a824bd97-6184-412e-8d3c-de032d5a9f2e",
   "metadata": {},
   "source": [
    "We can use `head(n)` method to show the heading of this data frame. `n` is the number of rows and its default value is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a91a051c-312f-40aa-8f6c-cf5ec592573e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(event_date='03/04/2019 19:48:06', location_id='loc0', temp_celcius='29'),\n",
       " Row(event_date='03/04/2019 19:53:06', location_id='loc0', temp_celcius='27'),\n",
       " Row(event_date='03/04/2019 19:58:06', location_id='loc0', temp_celcius='28'),\n",
       " Row(event_date='03/04/2019 20:03:06', location_id='loc0', temp_celcius='30'),\n",
       " Row(event_date='03/04/2019 20:08:06', location_id='loc0', temp_celcius='27')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979637cf-4b9c-4e86-ae4f-779cc2cbea03",
   "metadata": {},
   "source": [
    "If we want to show the data in a tabular format, we can use `.show(n)` method. `n` is the number of rows and its default value is 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dad58bc1-02c4-48f4-bab0-f1b19e99df80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "|03/04/2019 20:13:06|       loc0|          27|\n",
      "|03/04/2019 20:18:06|       loc0|          27|\n",
      "|03/04/2019 20:23:06|       loc0|          29|\n",
      "|03/04/2019 20:28:06|       loc0|          32|\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "|03/04/2019 20:38:06|       loc0|          32|\n",
      "|03/04/2019 20:43:06|       loc0|          28|\n",
      "|03/04/2019 20:48:06|       loc0|          28|\n",
      "|03/04/2019 20:53:06|       loc0|          32|\n",
      "|03/04/2019 20:58:06|       loc0|          34|\n",
      "|03/04/2019 21:03:06|       loc0|          33|\n",
      "|03/04/2019 21:08:06|       loc0|          27|\n",
      "|03/04/2019 21:13:06|       loc0|          28|\n",
      "|03/04/2019 21:18:06|       loc0|          33|\n",
      "|03/04/2019 21:23:06|       loc0|          28|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fea913c-e5cf-48e2-8b50-20b291836a35",
   "metadata": {},
   "source": [
    "To know the number of rows in the DataFrame, there is a useful method called `count()` that performs a count on the rows in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a52dc907-322a-4d24-aafc-50fca40abfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9286a75a-b359-48a6-bc9b-04804b77baa4",
   "metadata": {},
   "source": [
    "One of the useful methods in DataFrame API is `printSchema()` that prints out the schema in the tree format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13b3df70-ed2c-4daa-9b8b-006e78585879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_date: string (nullable = true)\n",
      " |-- location_id: string (nullable = true)\n",
      " |-- temp_celcius: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()\n",
    "#nullable means there is null value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa53a4e-1bb6-4070-a0fd-a05718f0e8f1",
   "metadata": {},
   "source": [
    "### Rename column names\n",
    "\n",
    "Now, let's load another file. In the data folder, we have another file called `utilization.csv`. This file does not have a header row. If we want to use the csv file schema, Spark provides an option to infer the columns' data types automatically. The following cells show how we can work with this type of csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed11d9d6-8ba3-478b-a7c6-bb3f899db332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Syaidatul Syafira\\\\OneDrive - studentupmedumy.onmicrosoft.com\\\\Desktop\\\\DA\\\\Big Data Analytics with Apache Spark\\\\Apache Spark SC\\\\data'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAIN_DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c014c622-83c5-4b05-b049-421a25d4d6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = MAIN_DIRECTORY + \"/utilization.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9514f73-5226-422f-9159-255707be91d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since this file there is no header name \n",
    "df2 = spark.read.format(\"csv\").option(\"header\",\"false\").option(\"inferSchema\",\"true\").load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4249cfd5-ddb0-4b72-abc8-9b7ab58bbd0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb8fcc2d-f7a0-443d-81a5-efe5d6a9c980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='03/05/2019 08:06:14', _c1=100, _c2=0.57, _c3=0.51, _c4=47),\n",
       " Row(_c0='03/05/2019 08:11:14', _c1=100, _c2=0.47, _c3=0.62, _c4=43),\n",
       " Row(_c0='03/05/2019 08:16:14', _c1=100, _c2=0.56, _c3=0.57, _c4=62),\n",
       " Row(_c0='03/05/2019 08:21:14', _c1=100, _c2=0.57, _c3=0.56, _c4=50),\n",
       " Row(_c0='03/05/2019 08:26:14', _c1=100, _c2=0.35, _c3=0.46, _c4=43)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc03c5-64f0-42ba-a6ec-f4eb8186b899",
   "metadata": {},
   "source": [
    "As you can see, we have five rows, but we do not have column names. Because we did not specify a header. So Spark just created column names. Basically used a pattern `_c#`.\n",
    "_c0 means 1st column, _c1 is 2nd column and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b783ac-fd90-45a9-a937-4d05cdc12809",
   "metadata": {},
   "source": [
    "Spark allows us to rename column using 'withColumnRenamed()' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac756c1c-e323-4619-8ab8-1eb82fdee995",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumnRenamed('_c0', 'event_datetime')\\\n",
    "         .withColumnRenamed('_c1','server_id')\\\n",
    "         .withColumnRenamed('_c2','cpu_utilization')\\\n",
    "         .withColumnRenamed('_c3','free_memory')\\\n",
    "         .withColumnRenamed('_c4','session_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42c1f273-4d65-4f6d-8c85-845e19fe7235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|     event_datetime|server_id|cpu_utilization|free_memory|session_count|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|03/05/2019 08:06:14|      100|           0.57|       0.51|           47|\n",
      "|03/05/2019 08:11:14|      100|           0.47|       0.62|           43|\n",
      "|03/05/2019 08:16:14|      100|           0.56|       0.57|           62|\n",
      "|03/05/2019 08:21:14|      100|           0.57|       0.56|           50|\n",
      "|03/05/2019 08:26:14|      100|           0.35|       0.46|           43|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f6c6a3-b844-4e59-a5b2-cc42a5cf0dce",
   "metadata": {},
   "source": [
    "To check data type of the dataframe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0fa6716-1529-4524-82db-6b320fb4a5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_datetime: string (nullable = true)\n",
      " |-- server_id: integer (nullable = true)\n",
      " |-- cpu_utilization: double (nullable = true)\n",
      " |-- free_memory: double (nullable = true)\n",
      " |-- session_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()\n",
    "# double means two decimal place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95de98-1c2a-46de-8b9f-4a65c3dd5f59",
   "metadata": {},
   "source": [
    "Another useful method in DataFrame API is `describe()` that computes basic statistics for numeric and string columns.\n",
    "\n",
    "This include count, mean, stddev, min, and max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2f82a9f-2be7-4227-a718-549b1aada97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|   cpu_utilization|\n",
      "+-------+------------------+\n",
      "|  count|            500000|\n",
      "|   mean|0.6205177400000196|\n",
      "| stddev|0.1587517387291281|\n",
      "|    min|              0.22|\n",
      "|    max|               1.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.describe('cpu_utilization').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe42cd58-6ef2-4611-9edf-55d7750db2e4",
   "metadata": {},
   "source": [
    "On average, 62% of CPU is utilised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a747b53-1706-44f2-953c-c4babe1ad8e8",
   "metadata": {},
   "source": [
    "To show statistics on all columns include string and numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c75a87d3-4749-437d-85ba-32672bcf2972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|     event_datetime|         server_id|   cpu_utilization|       free_memory|    session_count|\n",
      "+-------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|             500000|            500000|            500000|            500000|           500000|\n",
      "|   mean|               null|             124.5|0.6205177400000196|0.3791281000000497|         69.59616|\n",
      "| stddev|               null|14.430884120551388|0.1587517387291281|0.1583093127837626|14.85067669635274|\n",
      "|    min|03/05/2019 08:06:14|               100|              0.22|               0.0|               32|\n",
      "|    max|04/09/2019 01:22:46|               149|               1.0|              0.78|              105|\n",
      "+-------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bf91d2-3b31-47c7-ae29-1ec026a7fd28",
   "metadata": {},
   "source": [
    "### Load a JSON file into a DataFrame\n",
    "In the following cell, we are trying to load a JSON file into a DataFrame by using `spark.read` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d21c52d-ad05-4077-a99f-8c272f7dc808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Syaidatul Syafira\\\\OneDrive - studentupmedumy.onmicrosoft.com\\\\Desktop\\\\DA\\\\Big Data Analytics with Apache Spark\\\\Apache Spark SC\\\\data'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAIN_DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14293387-0edc-47cd-97b9-67fe504fa9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = MAIN_DIRECTORY + \"/utilization.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5a081a8-1ba6-451b-b4e0-c6bd53bf914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.read.format(\"json\").load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7e0c9c3-ba3e-4b05-b242-18c95bdb9bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.77|03/16/2019 17:21:40|       0.22|      115|           58|\n",
      "|           0.53|03/16/2019 17:26:40|       0.23|      115|           64|\n",
      "|            0.6|03/16/2019 17:31:40|       0.19|      115|           82|\n",
      "|           0.46|03/16/2019 17:36:40|       0.32|      115|           60|\n",
      "|           0.77|03/16/2019 17:41:40|       0.49|      115|           84|\n",
      "|           0.62|03/16/2019 17:46:40|       0.31|      115|           73|\n",
      "|           0.71|03/16/2019 17:51:40|       0.54|      115|           67|\n",
      "|           0.67|03/16/2019 17:56:40|       0.54|      115|           83|\n",
      "|           0.72|03/16/2019 18:01:40|       0.26|      115|           68|\n",
      "|           0.62|03/16/2019 18:06:40|       0.52|      115|           60|\n",
      "|           0.58|03/16/2019 18:11:40|       0.23|      115|           60|\n",
      "|           0.51|03/16/2019 18:16:40|       0.35|      115|           62|\n",
      "|           0.54|03/16/2019 18:21:40|       0.33|      115|           78|\n",
      "|           0.84|03/16/2019 18:26:40|       0.35|      115|           66|\n",
      "|           0.65|03/16/2019 18:31:40|       0.51|      115|           89|\n",
      "|            0.8|03/16/2019 18:36:40|       0.25|      115|           76|\n",
      "|           0.66|03/16/2019 18:41:40|       0.41|      115|           87|\n",
      "|           0.67|03/16/2019 18:46:40|       0.36|      115|           62|\n",
      "|           0.63|03/16/2019 18:51:40|       0.54|      115|           67|\n",
      "|           0.51|03/16/2019 18:56:40|       0.51|      115|           58|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "601a17eb-ae75-4021-9318-d8d1592bcd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1b971-4cfc-41cc-9356-dba8110154bf",
   "metadata": {},
   "source": [
    "Now, what you will notice here is we did not have to change column names.That is because in JSON, you specify key-value pairs. For example, there was a row that has `cpu_utilization` equals to 0.77, that corresponds to the first row. This row also has a key-value pair with `free_memory` equals to 0.22 and `server_id` equals to 115."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d32149e-903a-4808-9fa7-fb9b6e96daf3",
   "metadata": {},
   "source": [
    "Apache Spark provides an attribute called `columns`, to show the list of a DataFrame's columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd89ce4c-e456-4066-a29f-1c62cc8c473e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cpu_utilization',\n",
       " 'event_datetime',\n",
       " 'free_memory',\n",
       " 'server_id',\n",
       " 'session_count']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788c1214-13a1-4ed2-bf5b-936d489d2309",
   "metadata": {},
   "source": [
    "Sometimes we want to work with a subset of data. For example, we have 500000 rows of data in this DataFrame. Although they are not too many rows, it may be more than you want to work with at any particular time. And you would rather work with a sample of the data. To do that, you can use `sample` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa1df339-dc87-40a9-a308-9d2bea0a6c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_sample = df3.sample(False, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aff60ce2-d4d6-4d0d-af9e-1abe9c15fcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50045"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_sample.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520bba37-1c23-4a1b-8cf3-37735db7b82f",
   "metadata": {},
   "source": [
    "DataFrame API provides a method called `sort()` to sort the rows based on one or more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ffab50f-2ef6-429b-bcfd-76689a13f659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.29|03/05/2019 09:46:14|        0.6|      100|           44|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n",
      "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n",
      "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n",
      "|           0.32|03/05/2019 08:56:14|       0.37|      100|           47|\n",
      "|           0.62|03/05/2019 09:01:14|       0.59|      100|           60|\n",
      "|           0.66|03/05/2019 09:06:14|       0.72|      100|           57|\n",
      "|           0.54|03/05/2019 09:11:14|       0.54|      100|           44|\n",
      "|           0.29|03/05/2019 09:16:14|        0.4|      100|           47|\n",
      "|           0.43|03/05/2019 09:21:14|       0.68|      100|           66|\n",
      "|           0.49|03/05/2019 09:26:14|       0.66|      100|           65|\n",
      "|           0.64|03/05/2019 09:31:14|       0.55|      100|           66|\n",
      "|           0.42|03/05/2019 09:36:14|        0.6|      100|           42|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3_sort = df3.sort('server_id').show()\n",
    "#ascending as default\n",
    "#smallest to biggest number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed5e083-db36-4926-af76-1a78b6637a53",
   "metadata": {},
   "source": [
    "If we want to sort the rows based on more that one coulmn, we can specify the list of columns and sorting order by using the following syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fa7cf9e-8309-4b28-8718-52374f82fd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.74|04/09/2019 01:22:46|       0.19|      149|           85|\n",
      "|           0.83|04/09/2019 01:22:44|       0.21|      148|           69|\n",
      "|            0.4|04/09/2019 01:22:41|       0.42|      147|           65|\n",
      "|           0.62|04/09/2019 01:22:39|       0.13|      146|           92|\n",
      "|           0.77|04/09/2019 01:22:37|       0.12|      145|           88|\n",
      "|           0.78|04/09/2019 01:22:35|       0.46|      144|           64|\n",
      "|           0.37|04/09/2019 01:22:33|        0.4|      143|           59|\n",
      "|           0.77|04/09/2019 01:22:31|       0.27|      142|           68|\n",
      "|           0.44|04/09/2019 01:22:29|       0.59|      141|           54|\n",
      "|           0.63|04/09/2019 01:22:28|       0.21|      140|           60|\n",
      "|           0.67|04/09/2019 01:22:25|       0.34|      139|           69|\n",
      "|           0.32|04/09/2019 01:22:23|        0.6|      138|           39|\n",
      "|           0.57|04/09/2019 01:22:21|       0.32|      137|           87|\n",
      "|           0.41|04/09/2019 01:22:19|       0.33|      136|           74|\n",
      "|            0.5|04/09/2019 01:22:17|       0.63|      135|           71|\n",
      "|            0.5|04/09/2019 01:22:15|       0.34|      134|           58|\n",
      "|           0.87|04/09/2019 01:22:13|       0.43|      133|           77|\n",
      "|           0.72|04/09/2019 01:22:12|        0.4|      132|           49|\n",
      "|           0.57|04/09/2019 01:22:09|       0.38|      131|           58|\n",
      "|           0.51|04/09/2019 01:22:07|       0.25|      130|           61|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#one descending and one ascending, 1=true, 0=false\n",
    "#pass down the column names in []\n",
    "df3_sorted = df3.sort(['event_datetime','server_id'], ascending = [0,1]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a8c18-d1b4-4bcf-a2f0-ff622c9127cd",
   "metadata": {},
   "source": [
    "### Filtering using DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c31bb1-ab1e-4338-913a-b863f5be11a2",
   "metadata": {},
   "source": [
    "Now, let's take a look at how we can use DataFrame API to filter some of the rows in DataFrames.\n",
    "\n",
    "One of the DataFrames that we have created is `df1`, which stores location ID, and temperature measurement at a particular point and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f5d10e5-7cc2-4223-a691-b6c16b90e111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9072c2f7-029e-4a1d-9cbe-ba6f3183b5aa",
   "metadata": {},
   "source": [
    "If we want to filter rows based on their `location_id`, we can use `filter` command. `filter(condition)` filters rows using the given condition. `filter()` method essentially allows us to specify a `WHERE` clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7aed7dd2-b32a-42af-bacb-a3fe029b2e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "|03/04/2019 20:13:06|       loc0|          27|\n",
      "|03/04/2019 20:18:06|       loc0|          27|\n",
      "|03/04/2019 20:23:06|       loc0|          29|\n",
      "|03/04/2019 20:28:06|       loc0|          32|\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "|03/04/2019 20:38:06|       loc0|          32|\n",
      "|03/04/2019 20:43:06|       loc0|          28|\n",
      "|03/04/2019 20:48:06|       loc0|          28|\n",
      "|03/04/2019 20:53:06|       loc0|          32|\n",
      "|03/04/2019 20:58:06|       loc0|          34|\n",
      "|03/04/2019 21:03:06|       loc0|          33|\n",
      "|03/04/2019 21:08:06|       loc0|          27|\n",
      "|03/04/2019 21:13:06|       loc0|          28|\n",
      "|03/04/2019 21:18:06|       loc0|          33|\n",
      "|03/04/2019 21:23:06|       loc0|          28|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1['location_id'] == 'loc0').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d56dd5c-10a2-4fc3-b020-f021d396475d",
   "metadata": {},
   "source": [
    "If we want to count all the rows that are located in a specific `location_id`,we can specify the `count()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53099391-8006-40c9-b015-8d539e86c7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1['location_id']=='loc0').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb21c0b-ab24-4e5e-a8a8-c65d9b2bc653",
   "metadata": {},
   "source": [
    "Sometimes we only need to list one or two columns; in this case, we can use `select()` method that projects a set of expressions and returns a new DataFrame. Let's take a look at how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "046a776a-a99f-40b8-8086-ec238c29a02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|location_id|temp_celcius|\n",
      "+-----------+------------+\n",
      "|       loc0|          29|\n",
      "|       loc0|          27|\n",
      "|       loc0|          28|\n",
      "|       loc0|          30|\n",
      "|       loc0|          27|\n",
      "|       loc0|          27|\n",
      "|       loc0|          27|\n",
      "|       loc0|          29|\n",
      "|       loc0|          32|\n",
      "|       loc0|          35|\n",
      "|       loc0|          32|\n",
      "|       loc0|          28|\n",
      "|       loc0|          28|\n",
      "|       loc0|          32|\n",
      "|       loc0|          34|\n",
      "|       loc0|          33|\n",
      "|       loc0|          27|\n",
      "|       loc0|          28|\n",
      "|       loc0|          33|\n",
      "|       loc0|          28|\n",
      "+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select('location_id', 'temp_celcius').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e8010-ad19-430f-bf41-c1774ab34c9e",
   "metadata": {},
   "source": [
    "### Aggregation using DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236caedc-d124-4fa7-b9bc-3634c23ad3ab",
   "metadata": {},
   "source": [
    "Now, let's take a look at aggregating using the DataFrame API. In the following cell we will use `groupBy` method that groups the DataFrame using the specified columns, so we can run aggregation on them.\n",
    "\n",
    "Inserted count() to count each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d6b3f6c-3e84-4d64-8de7-24980a3c9e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|location_id|count|\n",
      "+-----------+-----+\n",
      "|     loc196| 1000|\n",
      "|     loc226| 1000|\n",
      "|     loc463| 1000|\n",
      "|     loc150| 1000|\n",
      "|     loc292| 1000|\n",
      "|     loc311| 1000|\n",
      "|      loc22| 1000|\n",
      "|     loc351| 1000|\n",
      "|     loc370| 1000|\n",
      "|     loc419| 1000|\n",
      "|      loc31| 1000|\n",
      "|     loc305| 1000|\n",
      "|      loc82| 1000|\n",
      "|      loc90| 1000|\n",
      "|     loc118| 1000|\n",
      "|     loc195| 1000|\n",
      "|     loc208| 1000|\n",
      "|      loc39| 1000|\n",
      "|      loc75| 1000|\n",
      "|     loc228| 1000|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('location_id').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784514f3-3e46-4e34-9d8b-bd11de507f04",
   "metadata": {},
   "source": [
    "If we want to sort the DataFrame, we can use `orderBy` that returns a new DataFrame sorted by the specified column(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c4bade2-1135-47db-9592-eecd80faadb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "|03/04/2019 20:13:06|       loc0|          27|\n",
      "|03/04/2019 20:18:06|       loc0|          27|\n",
      "|03/04/2019 20:23:06|       loc0|          29|\n",
      "|03/04/2019 20:28:06|       loc0|          32|\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "|03/04/2019 20:38:06|       loc0|          32|\n",
      "|03/04/2019 20:43:06|       loc0|          28|\n",
      "|03/04/2019 20:48:06|       loc0|          28|\n",
      "|03/04/2019 20:53:06|       loc0|          32|\n",
      "|03/04/2019 20:58:06|       loc0|          34|\n",
      "|03/04/2019 21:03:06|       loc0|          33|\n",
      "|03/04/2019 21:08:06|       loc0|          27|\n",
      "|03/04/2019 21:13:06|       loc0|          28|\n",
      "|03/04/2019 21:18:06|       loc0|          33|\n",
      "|03/04/2019 21:23:06|       loc0|          28|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.orderBy('location_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13d9204-0371-4bdb-aa19-9b2fc759df0a",
   "metadata": {},
   "source": [
    "To calculate the average temperature at each location, we can use `agg` operation. Let's take a look at the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c054a27b-8a2e-4574-9a1b-530c449105cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|location_id|avg(temp_celcius)|\n",
      "+-----------+-----------------+\n",
      "|       loc0|           29.176|\n",
      "|       loc1|           28.246|\n",
      "|      loc10|           25.337|\n",
      "|     loc100|           27.297|\n",
      "|     loc101|           25.317|\n",
      "|     loc102|           30.327|\n",
      "|     loc103|           25.341|\n",
      "|     loc104|           26.204|\n",
      "|     loc105|           26.217|\n",
      "|     loc106|           27.201|\n",
      "|     loc107|           33.268|\n",
      "|     loc108|           32.195|\n",
      "|     loc109|           24.138|\n",
      "|      loc11|           25.308|\n",
      "|     loc110|           26.239|\n",
      "|     loc111|           31.391|\n",
      "|     loc112|           33.359|\n",
      "|     loc113|           30.345|\n",
      "|     loc114|           29.261|\n",
      "|     loc115|           23.239|\n",
      "+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#groupby and sort different. group by is based aphabecticall and sort is ascending or descending\n",
    "df1.groupBy('location_id').agg({'temp_celcius':'mean'}).orderBy('location_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f3acb2-6275-48e2-ba76-68db1285b7c1",
   "metadata": {},
   "source": [
    "There are different aggregation function options, for example, if we want to have the maximum temperature in each location, we can write the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a81298c8-a450-432c-89a5-db155babea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|location_id|max(temp_celcius)|\n",
      "+-----------+-----------------+\n",
      "|      loc99|               40|\n",
      "|      loc98|               39|\n",
      "|      loc97|               38|\n",
      "|      loc96|               35|\n",
      "|      loc95|               40|\n",
      "|      loc94|               32|\n",
      "|      loc93|               31|\n",
      "|      loc92|               40|\n",
      "|      loc91|               37|\n",
      "|      loc90|               30|\n",
      "|       loc9|               39|\n",
      "|      loc89|               37|\n",
      "|      loc88|               32|\n",
      "|      loc87|               38|\n",
      "|      loc86|               30|\n",
      "|      loc85|               35|\n",
      "|      loc84|               33|\n",
      "|      loc83|               33|\n",
      "|      loc82|               34|\n",
      "|      loc81|               30|\n",
      "+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('location_id').agg({'temp_celcius':'max'}).orderBy('location_id', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b6af1a-b9b2-4637-9648-948d39791fe4",
   "metadata": {},
   "source": [
    "### Data Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72fb97c-6b11-4894-95a8-0b6be1f6ad24",
   "metadata": {},
   "source": [
    "Sometimes, we may want to use sampling, particularly when we have large data sets, and we are doing kind of an exploratory analysis. We want to get kind of an understanding at a high level of what the data is like. Sampling can be beneficial for doing quick operations. Now, let's see how we can take a sample, or a subset of that, but randomly. In PySpark, `sample()` method returns a sampled subset of this DataFrame, and it usually takes two parameters, `fraction` that specifies the fraction of rows to generate, range [0.0, 1.0]. The second parameter is `withReplacement`, which is a boolean parameter. Usually, we assign `false` to it, in this case, what that means is each time we pull a row out of our sampling, we don't put it back in, so we will never get duplicates, we will always get unique values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3850319f-4894-428c-bd9a-111d870e9274",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_sample1 = df1.sample(fraction=0.1, withReplacement=False)\n",
    "#fraction 0.1 means we pull 10% of whole data as sample\n",
    "#with replacement false, means after 10% pulled we dont put it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0cf1fc46-68e8-4829-b7eb-b1d0a967b25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49983"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_sample1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0a14c-4914-4a6d-bee6-cf904eebea2c",
   "metadata": {},
   "source": [
    "Now, let's run some simple descriptive statistics on our sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da2e5e98-2582-4ff8-9711-d4ffc00e3ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------+-----------------+\n",
      "|summary|         event_date|location_id|     temp_celcius|\n",
      "+-------+-------------------+-----------+-----------------+\n",
      "|  count|             500000|     500000|           500000|\n",
      "|   mean|               null|       null|        28.065484|\n",
      "| stddev|               null|       null|3.810122948123565|\n",
      "|    min|03/04/2019 19:48:06|       loc0|               21|\n",
      "|    max|03/08/2019 07:04:55|      loc99|               40|\n",
      "+-------+-------------------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.describe().show()\n",
    "\n",
    "#to know row in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48294889-a7bf-4da5-9463-f3a9403e0938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|location_id| avg(temp_celcius)|\n",
      "+-----------+------------------+\n",
      "|       loc0|28.846153846153847|\n",
      "|       loc1|28.329787234042552|\n",
      "|      loc10|25.902173913043477|\n",
      "|     loc100|27.394736842105264|\n",
      "|     loc101|              25.5|\n",
      "|     loc102|30.032967032967033|\n",
      "|     loc103| 25.35483870967742|\n",
      "|     loc104|26.273684210526316|\n",
      "|     loc105|              26.0|\n",
      "|     loc106| 27.22826086956522|\n",
      "+-----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_sample1.groupBy('location_id').agg({'temp_celcius':'mean'}).orderBy('location_id').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16836c0e-1123-41ee-bb42-a5e283fa7134",
   "metadata": {},
   "source": [
    "Now, let's compare these results to results of the original data set, the DataFrame `df1`, which has 500000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7696b98e-e1ab-4276-b8f4-158c5162951c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|location_id|avg(temp_celcius)|\n",
      "+-----------+-----------------+\n",
      "|       loc0|           29.176|\n",
      "|       loc1|           28.246|\n",
      "|      loc10|           25.337|\n",
      "|     loc100|           27.297|\n",
      "|     loc101|           25.317|\n",
      "|     loc102|           30.327|\n",
      "|     loc103|           25.341|\n",
      "|     loc104|           26.204|\n",
      "|     loc105|           26.217|\n",
      "|     loc106|           27.201|\n",
      "+-----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('location_id').agg({'temp_celcius':'mean'}).orderBy('location_id').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9322a63-3312-4cfd-9b88-fa6ebcd8b3eb",
   "metadata": {},
   "source": [
    "This shows that the sample datframe is representative from df1. Its representative because\n",
    "it's similar with original one "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ab8c5-bf7d-4b9c-a540-9687210009e4",
   "metadata": {},
   "source": [
    "As you can see, when we did the sampling and took 10% when we took the average of location zero, we got something that was about 29.4, but the actual is approximately 29.18. Therefore, we can see by sampling, we get very close to what the average is for the actual population. One of the things to consider is the size of the sample that we are drawing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f62e9db-554c-4345-8c7a-9742cbc86fcf",
   "metadata": {},
   "source": [
    "### Save Data from DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d983d233-aba6-4c4a-a1c9-ac56c144269b",
   "metadata": {},
   "source": [
    "Sometimes after we have been working with DataFrames and creating new DataFrames and running calculations and doing sampling, we might want to save our results out. To do this, we can use `write` object and specify the `csv()` method within that, and then specify a name or what we'd like to save. It saves the DataFrame to disk using the csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "821cd190-73ca-444a-8fa1-7bbf89d260df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.write.csv('df01.csv')\n",
    "\n",
    "#.csv is type of file you want to save and inside parameter is name of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc1d6df3-5d4a-4611-954c-8f901a4fe9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is 7EB8-72C1\n",
      "\n",
      " Directory of C:\\Users\\Syaidatul Syafira\\OneDrive - studentupmedumy.onmicrosoft.com\\Desktop\\DA\\Big Data Analytics with Apache Spark\\Apache Spark SC\\data\\df01.csv\n",
      "\n",
      "01/02/2023  09:43 PM    <DIR>          .\n",
      "01/02/2023  09:43 PM    <DIR>          ..\n",
      "01/02/2023  09:43 PM           120,244 .part-00000-6d89a06f-e670-4a89-9660-f358a71f1b2c-c000.csv.crc\n",
      "01/02/2023  09:43 PM                 8 ._SUCCESS.crc\n",
      "01/02/2023  09:43 PM        15,390,000 part-00000-6d89a06f-e670-4a89-9660-f358a71f1b2c-c000.csv\n",
      "01/02/2023  09:43 PM                 0 _SUCCESS\n",
      "               4 File(s)     15,510,252 bytes\n",
      "               2 Dir(s)   1,578,106,880 bytes free\n"
     ]
    }
   ],
   "source": [
    "! dir df01.csv\n",
    "# in windows, you would use dir command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6637a69b-59de-42f2-ad10-7d5cf1743f54",
   "metadata": {},
   "source": [
    "Now, what you will notice here is that `df1.csv` is not a single file. It is a directory. And what is in that directory is four different files with `csv` extensions, and that is because of the way Apache Spark works internally. Spark can break up DataFrames into partition subsets, and in this case, there were four partitions. Each partition has its own file. There is also a `_SUCCESS` flag that was written out. Now, let's list the contents of one of these files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "602b2770-9390-40c6-8392-a920eaabcf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid parameter - /part-00000-6d89a06f-e670-4a89-9660-f358a71f1b2c-c000.csv\n"
     ]
    }
   ],
   "source": [
    "! more df1.csv/part-00000-6d89a06f-e670-4a89-9660-f358a71f1b2c-c000.csv\n",
    "\n",
    " # for Windows, use more command "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba64e12-ddab-4ecc-8f9b-2c0b3d373fc0",
   "metadata": {},
   "source": [
    "To write the DataFrame in JSON format, you can use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "722903da-070b-4884-91a9-05115496fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.write.json('df01.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de74e0dd-a8b1-4cb8-bd05-b3de6f1f7806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is 7EB8-72C1\n",
      "\n",
      " Directory of C:\\Users\\Syaidatul Syafira\\OneDrive - studentupmedumy.onmicrosoft.com\\Desktop\\DA\\Big Data Analytics with Apache Spark\\Apache Spark SC\\data\\df01.json\n",
      "\n",
      "01/02/2023  09:43 PM    <DIR>          .\n",
      "01/02/2023  09:43 PM    <DIR>          ..\n",
      "01/02/2023  09:43 PM           311,652 .part-00000-98a7ace9-b70d-4dfb-b716-09e814630f3b-c000.json.crc\n",
      "01/02/2023  09:43 PM                 8 ._SUCCESS.crc\n",
      "01/02/2023  09:43 PM        39,890,000 part-00000-98a7ace9-b70d-4dfb-b716-09e814630f3b-c000.json\n",
      "01/02/2023  09:43 PM                 0 _SUCCESS\n",
      "               4 File(s)     40,201,660 bytes\n",
      "               2 Dir(s)   1,536,667,648 bytes free\n"
     ]
    }
   ],
   "source": [
    "! dir df01.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7613449a-7561-471b-a93a-33f183758d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid parameter - /part-00000-d58cea08-aad2-4357-a4d0-7aef461de51c-c000.json\n"
     ]
    }
   ],
   "source": [
    "! more df1.csv/part-00000-d58cea08-aad2-4357-a4d0-7aef461de51c-c000.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73a771b-3f39-4263-9ad4-92779474f4f1",
   "metadata": {},
   "source": [
    "## Querying DataFrames with SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f840f397-a7b3-46c8-a7ba-0e60b70cb31f",
   "metadata": {},
   "source": [
    "Up to now, we've been using the Spark DataFrame API to work with DataFrames. Now, we're going to switch gears and we're going to work with SQL. In particular, we're going to use Spark SQL for working with DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1424c95a-12e8-4b2c-9b2f-2fd810d3c8e7",
   "metadata": {},
   "source": [
    "In this part, we will use `utilization.json` that includes cpu utilization, the amount of free memory at a particular point in time, and then the number of sessions that are currently connected to the server at the particular point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56780a2b-d156-48d6-a6a3-53cc62e5bf51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Syaidatul Syafira\\\\OneDrive - studentupmedumy.onmicrosoft.com\\\\Desktop\\\\DA\\\\Big Data Analytics with Apache Spark\\\\Apache Spark SC\\\\data'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAIN_DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b024a65-4487-4758-8673-06a51320181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = MAIN_DIRECTORY+ \"/utilization.json\"\n",
    "df_util = spark.read.format('json').load(file_path)\n",
    "#CREATE NEW VARIABLE CALLED AS df_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3dc7eff3-8041-4e25-abe2-29866be07594",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path =MAIN_DIRECTORY+\"/utilization.csv\"\n",
    "df_util = spark.read.format('csv').option('inferSchema', 'true').load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "539fa9cd-2477-4988-a6b3-06f8ad846a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----+----+---+\n",
      "|                _c0|_c1| _c2| _c3|_c4|\n",
      "+-------------------+---+----+----+---+\n",
      "|03/05/2019 08:06:14|100|0.57|0.51| 47|\n",
      "|03/05/2019 08:11:14|100|0.47|0.62| 43|\n",
      "|03/05/2019 08:16:14|100|0.56|0.57| 62|\n",
      "|03/05/2019 08:21:14|100|0.57|0.56| 50|\n",
      "|03/05/2019 08:26:14|100|0.35|0.46| 43|\n",
      "|03/05/2019 08:31:14|100|0.41|0.58| 48|\n",
      "|03/05/2019 08:36:14|100|0.57|0.35| 58|\n",
      "|03/05/2019 08:41:14|100|0.41| 0.4| 58|\n",
      "|03/05/2019 08:46:14|100|0.53|0.35| 62|\n",
      "|03/05/2019 08:51:14|100|0.51| 0.6| 45|\n",
      "+-------------------+---+----+----+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "297ca010-90c3-402b-be92-a38f7319a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_util = df_util.withColumnRenamed('_c0', 'event_datetime')\\\n",
    "         .withColumnRenamed('_c1','server_id')\\\n",
    "         .withColumnRenamed('_c2','cpu_utilization')\\\n",
    "         .withColumnRenamed('_c3','free_memory')\\\n",
    "         .withColumnRenamed('_c4','session_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec293cef-d2c3-49d5-93ef-89afdd2e3d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_util.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f9f3f-064e-4f26-b8d0-c965927f9e5a",
   "metadata": {},
   "source": [
    "To work with SQL in Spark, we have to create a temporary view. And to do that, we specify the DataFrame, and then we call the method `createOrReplaceTempView()` and then we should specify a name for this table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6230026-21f7-4e56-9664-e83faf13a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_util.createOrReplaceTempView(\"utilization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62ab09e-c499-4f06-8d65-2083e86d1b12",
   "metadata": {},
   "source": [
    "Now, we have the ability to query on a table called utilization. We will create that by executing a SQL command in the Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c6f1d38-47d1-465f-ae2c-ea4d1e0a212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql = spark.sql(\"SELECT * FROM utilization LIMIT 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d09ac617-9f9b-41ce-8b51-961b91be58c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|     event_datetime|server_id|cpu_utilization|free_memory|session_count|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|03/05/2019 08:06:14|      100|           0.57|       0.51|           47|\n",
      "|03/05/2019 08:11:14|      100|           0.47|       0.62|           43|\n",
      "|03/05/2019 08:16:14|      100|           0.56|       0.57|           62|\n",
      "|03/05/2019 08:21:14|      100|           0.57|       0.56|           50|\n",
      "|03/05/2019 08:26:14|      100|           0.35|       0.46|           43|\n",
      "|03/05/2019 08:31:14|      100|           0.41|       0.58|           48|\n",
      "|03/05/2019 08:36:14|      100|           0.57|       0.35|           58|\n",
      "|03/05/2019 08:41:14|      100|           0.41|        0.4|           58|\n",
      "|03/05/2019 08:46:14|      100|           0.53|       0.35|           62|\n",
      "|03/05/2019 08:51:14|      100|           0.51|        0.6|           45|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c0f798-06f6-496a-92a5-df20a5149524",
   "metadata": {},
   "source": [
    "If we want to project on specific columns, we can do it in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1bc509b2-ef4c-4c69-8dd6-c83a270d27e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|server_id|free_memory|\n",
      "+---------+-----------+\n",
      "|      100|       0.51|\n",
      "|      100|       0.62|\n",
      "|      100|       0.57|\n",
      "|      100|       0.56|\n",
      "|      100|       0.46|\n",
      "|      100|       0.58|\n",
      "|      100|       0.35|\n",
      "|      100|        0.4|\n",
      "|      100|       0.35|\n",
      "|      100|        0.6|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT server_id, free_memory FROM utilization LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdbacef-bba2-4df5-80b1-3cdc123fd738",
   "metadata": {},
   "source": [
    "### Filtering DataFrames with SQL\n",
    "Next, we are going to take a look at how to filter DataFrames using Spark SQL.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7cb7e301-0311-4b3e-bd72-2043ece61534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|     event_datetime|server_id|cpu_utilization|free_memory|session_count|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|03/05/2019 08:07:44|      149|           0.74|       0.27|           66|\n",
      "|03/05/2019 08:12:44|      149|            0.9|       0.34|           85|\n",
      "|03/05/2019 08:17:44|      149|           0.59|       0.19|           84|\n",
      "|03/05/2019 08:22:44|      149|            0.6|       0.08|           81|\n",
      "|03/05/2019 08:27:44|      149|           0.83|       0.42|           73|\n",
      "|03/05/2019 08:32:44|      149|           0.75|       0.19|           84|\n",
      "|03/05/2019 08:37:44|      149|            0.9|       0.14|           92|\n",
      "|03/05/2019 08:42:44|      149|           0.67|       0.16|           88|\n",
      "|03/05/2019 08:47:44|      149|           0.91|       0.31|           71|\n",
      "|03/05/2019 08:52:44|      149|           0.82|       0.13|           72|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "spark.sql(\"SELECT * FROM utilization WHERE server_id = 149 LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ab1ffd37-0d05-44ba-a901-5dfa5e53469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|sid| sc|\n",
      "+---+---+\n",
      "|100| 62|\n",
      "|100| 58|\n",
      "|100| 58|\n",
      "|100| 62|\n",
      "|100| 60|\n",
      "|100| 57|\n",
      "|100| 66|\n",
      "|100| 65|\n",
      "|100| 66|\n",
      "|100| 63|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "spark.sql(\"SELECT server_id as sid, session_count as sc \\\n",
    "            FROM utilization WHERE session_count >50 LIMIT 10\").show()\n",
    "\n",
    "#\\ is to move as next row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5534b40e-9605-420a-931f-fea319c48688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|server_id|session_count|\n",
      "+---------+-------------+\n",
      "|      120|           80|\n",
      "|      120|           80|\n",
      "|      120|           80|\n",
      "|      120|           80|\n",
      "|      120|           80|\n",
      "|      120|           80|\n",
      "|      120|           80|\n",
      "|      120|           80|\n",
      "|      120|           80|\n",
      "|      120|           80|\n",
      "+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3\n",
    "spark.sql(\"SELECT server_id, session_count FROM utilization \\\n",
    "           WHERE session_count >50 AND server_id = 120 \\\n",
    "           ORDER BY session_count DESC \\\n",
    "           LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e70d05d-a6c6-4746-866a-13142df7a5c6",
   "metadata": {},
   "source": [
    "### Aggregation DataFrames with SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef755f9-772e-4989-806e-d28578966968",
   "metadata": {},
   "source": [
    "When we work with SQL in databases, we often use SQL to perform aggregations and the same holds true when working with SQL in Spark. Let's write some basic queries against the DataFrame and do a very simple aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ede65640-47ac-449a-aecf-976c8d45787c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  500000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "spark.sql(\"SELECT count(*) FROM utilization\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5ed84060-46d7-4f24-a369-cce429710e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  239659|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "spark.sql(\"SELECT count(*) FROM utilization WHERE session_count > 70\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4d542ca3-5601-435a-b4aa-96e4e0f42705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|server_id|count(1)|\n",
      "+---------+--------+\n",
      "|      148|    8027|\n",
      "|      137|    8248|\n",
      "|      133|    8583|\n",
      "|      108|    8375|\n",
      "|      101|    9808|\n",
      "|      115|    5284|\n",
      "|      126|    6365|\n",
      "|      103|    8744|\n",
      "|      128|    3719|\n",
      "|      122|    4885|\n",
      "|      111|    3093|\n",
      "|      140|    6163|\n",
      "|      132|    2048|\n",
      "|      146|    7072|\n",
      "|      142|    7084|\n",
      "|      139|    7383|\n",
      "|      120|    2733|\n",
      "|      117|    3605|\n",
      "|      112|    7425|\n",
      "|      127|    5961|\n",
      "+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3\n",
    "spark.sql(\"SELECT server_id, count(*) FROM utilization \\\n",
    "           WHERE session_count > 70 \\\n",
    "           GROUP BY server_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f2cadb7e-d4e4-40d9-b883-057cbd10827d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|server_id|count(1)|\n",
      "+---------+--------+\n",
      "|      101|    9808|\n",
      "|      113|    9418|\n",
      "|      145|    9304|\n",
      "|      103|    8744|\n",
      "|      102|    8586|\n",
      "|      133|    8583|\n",
      "|      108|    8375|\n",
      "|      149|    8288|\n",
      "|      137|    8248|\n",
      "|      148|    8027|\n",
      "|      123|    7918|\n",
      "|      118|    7913|\n",
      "|      112|    7425|\n",
      "|      139|    7383|\n",
      "|      104|    7366|\n",
      "|      142|    7084|\n",
      "|      121|    7084|\n",
      "|      146|    7072|\n",
      "|      126|    6365|\n",
      "|      144|    6220|\n",
      "+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 4\n",
    "spark.sql(\"SELECT server_id, count(*) FROM utilization \\\n",
    "           WHERE session_count > 70 \\\n",
    "           GROUP BY server_id \\\n",
    "           ORDER BY count(*) DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "527247c7-6b6d-4dd4-833e-3a2a3ec53302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+------------------+----------------------------+\n",
      "|server_id|min(session_count)|max(session_count)|round(avg(session_count), 2)|\n",
      "+---------+------------------+------------------+----------------------------+\n",
      "|      101|                71|               105|                       87.67|\n",
      "|      113|                71|               103|                       86.96|\n",
      "|      145|                71|               103|                       86.98|\n",
      "|      103|                71|               101|                       85.76|\n",
      "|      102|                71|               101|                       85.71|\n",
      "|      133|                71|               100|                       85.47|\n",
      "|      108|                71|               100|                       85.12|\n",
      "|      149|                71|                99|                       84.96|\n",
      "|      137|                71|                99|                       85.01|\n",
      "|      148|                71|                99|                        84.7|\n",
      "|      123|                71|                98|                       84.53|\n",
      "|      118|                71|                98|                       84.66|\n",
      "|      112|                71|                97|                       83.55|\n",
      "|      139|                71|                96|                       83.33|\n",
      "|      104|                71|                96|                       83.35|\n",
      "|      142|                71|                95|                        82.9|\n",
      "|      121|                71|                95|                       82.89|\n",
      "|      146|                71|                95|                       82.95|\n",
      "|      126|                71|                93|                       81.56|\n",
      "|      144|                71|                92|                       81.38|\n",
      "+---------+------------------+------------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 5\n",
    "spark.sql(\"SELECT server_id, min(session_count), max(session_count), \\\n",
    "           round(avg(session_count),2) \\\n",
    "           FROM utilization \\\n",
    "           WHERE session_count > 70 \\\n",
    "           GROUP BY server_id \\\n",
    "           ORDER BY count(*) DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47126a52-cc4c-4f66-b810-a5d1e5cafc15",
   "metadata": {},
   "source": [
    "### Joining DataFrames with SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c935b93-9010-4709-9ee2-a7a741c62796",
   "metadata": {},
   "source": [
    "One of the most useful features of SQL is the ability to join tables. We can join in Spark SQL as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c0ff62-c212-4f16-b061-4ea0ae7c2774",
   "metadata": {},
   "source": [
    "First, we are going to create another temporary table based on the `server_names.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a47723d7-eacd-42e1-8365-9b270115c311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Syaidatul Syafira\\\\OneDrive - studentupmedumy.onmicrosoft.com\\\\Desktop\\\\DA\\\\Big Data Analytics with Apache Spark\\\\Apache Spark SC\\\\data'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAIN_DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8b6c7d51-c184-4b5f-9694-2572009b38ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = MAIN_DIRECTORY + \"/server_names.csv\"\n",
    "df_servers = spark.read.csv(file_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "70279ae2-2d46-4747-b4d4-d04485b73baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|server_id|server_name|\n",
      "+---------+-----------+\n",
      "|      100| Server 100|\n",
      "|      101| Server 101|\n",
      "|      102| Server 102|\n",
      "|      103| Server 103|\n",
      "|      104| Server 104|\n",
      "|      105| Server 105|\n",
      "|      106| Server 106|\n",
      "|      107| Server 107|\n",
      "|      108| Server 108|\n",
      "|      109| Server 109|\n",
      "|      110| Server 110|\n",
      "|      111| Server 111|\n",
      "|      112| Server 112|\n",
      "|      113| Server 113|\n",
      "|      114| Server 114|\n",
      "|      115| Server 115|\n",
      "|      116| Server 116|\n",
      "|      117| Server 117|\n",
      "|      118| Server 118|\n",
      "|      119| Server 119|\n",
      "+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_servers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c454e6cb-6ab6-420f-87fb-d5a308fe0160",
   "metadata": {},
   "source": [
    "to create a temporary view. And to do that, we specify the DataFrame, and then we call the method createOrReplaceTempView()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "711626c5-0b25-4648-a780-beae7988fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_servers.createOrReplaceTempView('server_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4088a9-7300-44c2-a126-0de637e1d085",
   "metadata": {},
   "source": [
    "Now, let's quickly do a check on `server_id` in `utilization` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "659500f2-5935-424d-9cf6-d96dffd2167c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|server_id|\n",
      "+---------+\n",
      "|      100|\n",
      "|      101|\n",
      "|      102|\n",
      "|      103|\n",
      "|      104|\n",
      "|      105|\n",
      "|      106|\n",
      "|      107|\n",
      "|      108|\n",
      "|      109|\n",
      "|      110|\n",
      "|      111|\n",
      "|      112|\n",
      "|      113|\n",
      "|      114|\n",
      "|      115|\n",
      "|      116|\n",
      "|      117|\n",
      "|      118|\n",
      "|      119|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT DISTINCT server_id FROM utilization ORDER BY server_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b5b3c-73e8-4b7e-bae6-cbe9c631f676",
   "metadata": {},
   "source": [
    "Now, let's see what the minimum and maximum of server_id is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "99167d9a-2efe-431c-a203-62561cb8a4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|min(server_id)|max(server_id)|\n",
      "+--------------+--------------+\n",
      "|           100|           149|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT min(server_id), max(server_id) FROM utilization\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2093056b-0cdc-4777-ab2a-8868606ef65d",
   "metadata": {},
   "source": [
    "JOIN TWO TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ae2d5c89-947b-4446-8148-193aa3510c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------+\n",
      "|server_id|server_name|session_count|\n",
      "+---------+-----------+-------------+\n",
      "|      100| Server 100|           47|\n",
      "|      100| Server 100|           43|\n",
      "|      100| Server 100|           62|\n",
      "|      100| Server 100|           50|\n",
      "|      100| Server 100|           43|\n",
      "|      100| Server 100|           48|\n",
      "|      100| Server 100|           58|\n",
      "|      100| Server 100|           58|\n",
      "|      100| Server 100|           62|\n",
      "|      100| Server 100|           45|\n",
      "|      100| Server 100|           47|\n",
      "|      100| Server 100|           60|\n",
      "|      100| Server 100|           57|\n",
      "|      100| Server 100|           44|\n",
      "|      100| Server 100|           47|\n",
      "|      100| Server 100|           66|\n",
      "|      100| Server 100|           65|\n",
      "|      100| Server 100|           66|\n",
      "|      100| Server 100|           42|\n",
      "|      100| Server 100|           63|\n",
      "+---------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join = spark.sql(\"SELECT u.server_id, sn.server_name, u.session_count \\\n",
    "                     FROM utilization u \\\n",
    "                     INNER JOIN server_name sn \\\n",
    "                     ON sn.server_id = u.server_id\")\n",
    "df_join.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56df7a9-0897-4b2b-a67f-0b12fc08e3c6",
   "metadata": {},
   "source": [
    "### De-Duplicating with DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c17570b-3b02-4835-b294-7145ab553f07",
   "metadata": {},
   "source": [
    "When we're working with Data Frames, Spark provides some ways to de-duplicate data. So, let's take a look at how to do that. In this part also we will learn how we can create small data sets to work within the Jupiter Notebook session. Before doing anything, please restart the Jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5242edb9-9cff-4ff2-9797-18059efa7f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d08e29dc-2c63-4623-82f4-a488dd649d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc =SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8ff5fb-a5d9-4869-ba58-62444453e615",
   "metadata": {},
   "source": [
    "`sc` stands for `SparkContext`. It is a global variable that gives us access to the Spark Context. Here, what we want to do is create a DataFrame, and to do that, we will use `parallelize` method that creates a parallelized data structure. Spark automatically parallelize DataFrames. But now we are going to create this data manually, so we are specifying `parallelized` explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e44c84e7-160c-49a8-81ca-6cf1f57f56da",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([Row(server_name='Server 101', cpu_utilization=85, session_count=80),\n",
    "                      Row(server_name='Server 101', cpu_utilization=80, session_count=90),\n",
    "                      Row(server_name='Server 102', cpu_utilization=85, session_count=80),\n",
    "                      Row(server_name='Server 102', cpu_utilization=85, session_count=80)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbf51306-943b-4978-be89-9cd441346e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa8b1c2-82e8-43c8-a993-66f2916518fe",
   "metadata": {},
   "source": [
    "`toDF()` turns that parallelized data structure to into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b327543-e1e5-4375-addd-2f367af9518e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n",
      "|server_name|cpu_utilization|session_count|\n",
      "+-----------+---------------+-------------+\n",
      "| Server 101|             85|           80|\n",
      "| Server 101|             80|           90|\n",
      "| Server 102|             85|           80|\n",
      "| Server 102|             85|           80|\n",
      "+-----------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dup = rdd.toDF()\n",
    "df_dup.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de6d099-163f-455b-b700-810510c0ef93",
   "metadata": {},
   "source": [
    "Now, we are going to drop duplicates. To do that we can use `drop_duplicates()` method which returns a new DataFrame with duplicate rows removed, optionally only considering certain columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3e45a57-2ffb-491c-a9d1-4cb32bb96078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n",
      "|server_name|cpu_utilization|session_count|\n",
      "+-----------+---------------+-------------+\n",
      "| Server 101|             85|           80|\n",
      "| Server 102|             85|           80|\n",
      "| Server 101|             80|           90|\n",
      "+-----------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  df_dup.drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c59d9b0-4523-4ad9-9ca0-4d8642e89507",
   "metadata": {},
   "source": [
    "If we want to drop any time there is a duplicate in one of the columns, we can do that as well. Let's take a look at the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6159e785-cf77-4986-b4a5-bd39c6d76b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n",
      "|server_name|cpu_utilization|session_count|\n",
      "+-----------+---------------+-------------+\n",
      "| Server 101|             85|           80|\n",
      "| Server 102|             85|           80|\n",
      "+-----------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " df_dup.drop_duplicates(['server_name']).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c884f71-9246-4de0-b448-2619444ab654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n",
      "|server_name|cpu_utilization|session_count|\n",
      "+-----------+---------------+-------------+\n",
      "| Server 101|             80|           90|\n",
      "| Server 101|             85|           80|\n",
      "| Server 102|             85|           80|\n",
      "+-----------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " df_dup.drop_duplicates(['server_name', 'cpu_utilization']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73155c9f-6d81-46c8-8446-05b68227ed20",
   "metadata": {},
   "source": [
    "### Working with null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f9a9f-6254-4d75-9f46-e85f246dc83a",
   "metadata": {},
   "source": [
    "It is not uncommon to have data missing from DataFrame. When we are working with SQL, we are used to work with nulls. When we working with DataFrames, the absence of data is indicated by an NA. So in this section, we are going to look how we can work with NAs and Nulls using DataFrames and Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc6c0ee-89b7-4cfb-ba4d-f6883265c3c1",
   "metadata": {},
   "source": [
    "In this section, we are importing a couple of things, we have not seen before. Let's take a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dece318-992e-4d62-b592-6f51b1dd1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit #allows us to create a literal column for a dataframe\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6117b656-4813-4e36-95a4-a600674df38a",
   "metadata": {},
   "source": [
    "Now, we are going to add a column and set that column's values equall to null or NA. In this case, we will use `lit()` function that is a way for us to interact with column literals in PySpark. Spark SQL functions lit() is used to add a new column by assigning a literal or constant value to Spark DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd8b8fc9-02c7-4cc8-8c74-a115af9d4d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rdd.toDF()\n",
    "df_na = df.withColumn('na_col', lit(None).cast(StringType()))\n",
    "\n",
    "#we rite NONE so it came out as null because we did not enter a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70fcbf37-2980-4c20-9b1e-7cd267bc6d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| Server 101|             85|           80|  null|\n",
      "| Server 101|             80|           90|  null|\n",
      "| Server 102|             85|           80|  null|\n",
      "| Server 102|             85|           80|  null|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_na.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0b863f-caa8-4a73-ac93-174b0cae225d",
   "metadata": {},
   "source": [
    "Now, one of the things that we can do is globally replace all nulls or NAs with some value. And we can do that with `fillna()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2de1a33-241e-40ab-bb58-3c11d290c3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| Server 101|             85|           80|     A|\n",
      "| Server 101|             80|           90|     A|\n",
      "| Server 102|             85|           80|     A|\n",
      "| Server 102|             85|           80|     A|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_na.fillna('A').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c6a031-6fe3-405a-914e-4b791c4526a8",
   "metadata": {},
   "source": [
    "Now, Let's create a DataFrame that has versions both with the nulls and with the As."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c65e36c6-b7a7-4338-8409-d7365fdd33d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union = df_na.fillna('A').union(df_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c81122a-5b15-47c7-bacb-3401a2ec59c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| Server 101|             85|           80|     A|\n",
      "| Server 101|             80|           90|     A|\n",
      "| Server 102|             85|           80|     A|\n",
      "| Server 102|             85|           80|     A|\n",
      "| Server 101|             85|           80|  null|\n",
      "| Server 101|             80|           90|  null|\n",
      "| Server 102|             85|           80|  null|\n",
      "| Server 102|             85|           80|  null|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_union.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdd122e-7a10-4b29-b584-bf951779d0e6",
   "metadata": {},
   "source": [
    "Now we can drop only rows with NAs in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94d0cb68-ea0e-42de-bf73-435c9675ceaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| Server 101|             85|           80|     A|\n",
      "| Server 101|             80|           90|     A|\n",
      "| Server 102|             85|           80|     A|\n",
      "| Server 102|             85|           80|     A|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_union.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574ee8b7-fd9f-4e8c-8dc0-9f7f183f8df0",
   "metadata": {},
   "source": [
    "Well, let's see how we can do that with Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "897adac3-c5f7-4fb3-ab30-5f2f2c799693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union.createOrReplaceTempView('na_table')\n",
    "\n",
    "#creating temporary table for SQL in Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "722ea1f7-c41b-43da-afdf-c481c6881748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| Server 101|             85|           80|  null|\n",
      "| Server 101|             80|           90|  null|\n",
      "| Server 102|             85|           80|  null|\n",
      "| Server 102|             85|           80|  null|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM na_table WHERE na_col IS NULL').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc137097-f9b8-469b-a55d-3e17d454b80f",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis with DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df536095-5fea-46ca-bbbd-ad0f63996c0b",
   "metadata": {},
   "source": [
    "DataFrame API provides some tools for some higher level tasks like exploratory data analysis. In this section, we are going to learn how to use DataFrame API for doing some basic EDA with the utilization DataFrame. First, let's take a look at this DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c686790a-9758-497f-83ee-cd7f2eb7dc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|     event_datetime|server_id|cpu_utilization|free_memory|session_count|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|03/05/2019 08:06:14|      100|           0.57|       0.51|           47|\n",
      "|03/05/2019 08:11:14|      100|           0.47|       0.62|           43|\n",
      "|03/05/2019 08:16:14|      100|           0.56|       0.57|           62|\n",
      "|03/05/2019 08:21:14|      100|           0.57|       0.56|           50|\n",
      "|03/05/2019 08:26:14|      100|           0.35|       0.46|           43|\n",
      "|03/05/2019 08:31:14|      100|           0.41|       0.58|           48|\n",
      "|03/05/2019 08:36:14|      100|           0.57|       0.35|           58|\n",
      "|03/05/2019 08:41:14|      100|           0.41|        0.4|           58|\n",
      "|03/05/2019 08:46:14|      100|           0.53|       0.35|           62|\n",
      "|03/05/2019 08:51:14|      100|           0.51|        0.6|           45|\n",
      "|03/05/2019 08:56:14|      100|           0.32|       0.37|           47|\n",
      "|03/05/2019 09:01:14|      100|           0.62|       0.59|           60|\n",
      "|03/05/2019 09:06:14|      100|           0.66|       0.72|           57|\n",
      "|03/05/2019 09:11:14|      100|           0.54|       0.54|           44|\n",
      "|03/05/2019 09:16:14|      100|           0.29|        0.4|           47|\n",
      "|03/05/2019 09:21:14|      100|           0.43|       0.68|           66|\n",
      "|03/05/2019 09:26:14|      100|           0.49|       0.66|           65|\n",
      "|03/05/2019 09:31:14|      100|           0.64|       0.55|           66|\n",
      "|03/05/2019 09:36:14|      100|           0.42|        0.6|           42|\n",
      "|03/05/2019 09:41:14|      100|           0.55|       0.59|           63|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85e3f545-31d8-44ed-aa9b-630961ff8807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_util.count() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edc2ae9-170c-42a3-9aaa-310872aa44a6",
   "metadata": {},
   "source": [
    "One of the useful methods for doing exploratory data analysis is `.describe()`. Let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cf9e570-2705-41ab-9f2b-d5faf0f3d72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|     event_datetime|         server_id|   cpu_utilization|       free_memory|    session_count|\n",
      "+-------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|             500000|            500000|            500000|            500000|           500000|\n",
      "|   mean|               null|             124.5|0.6205177400000196|0.3791281000000497|         69.59616|\n",
      "| stddev|               null|14.430884120551388|0.1587517387291281|0.1583093127837626|14.85067669635274|\n",
      "|    min|03/05/2019 08:06:14|               100|              0.22|               0.0|               32|\n",
      "|    max|04/09/2019 01:22:46|               149|               1.0|              0.78|              105|\n",
      "+-------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0201067a-febb-4371-a071-b8f22911e47c",
   "metadata": {},
   "source": [
    "`.describe()` actually produces another DataFrame with summary statistics about the DataFrame. For example, in this case, we see that there are several columns; there is a summary column, followed by the name of a column in the original DataFrame. For each of those columns in the original DataFrame, we have the same statistics that are calculated.\n",
    "Using `.describe()`  is an excellent way to get a high-level view of what a data set might be like.\n",
    "\n",
    "Another statistics we often want to know, is there a correlation between two of the variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcd2a4f6-09ec-44a1-8efd-f7ab36dc5eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_datetime: string (nullable = true)\n",
      " |-- server_id: integer (nullable = true)\n",
      " |-- cpu_utilization: double (nullable = true)\n",
      " |-- free_memory: double (nullable = true)\n",
      " |-- session_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b1470cf-5007-4c92-ae6c-f81d875702ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4704771573080856"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_util.stat.corr('cpu_utilization', 'free_memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75164c30-5b5d-41f0-88c6-78e527412516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5008320848876534"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_util.stat.corr('session_count','free_memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc2c32a-0ea2-4386-997e-c80dcfac58a5",
   "metadata": {},
   "source": [
    "Sometimes we want to know how frequent are some items, what are the most frequently occurring items?\n",
    "\n",
    "There is a method called `.freq()` items for frequent items, which we can use with the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55f55e81-9ccd-4630-a301-26699b531ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "| server_id_freqItems|\n",
      "+--------------------+\n",
      "|[146, 137, 101, 1...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_freqItems = df_util.stat.freqItems(['server_id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef3233a6-353d-4046-adaa-13310f088d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+\n",
      "| server_id_freqItems|session_count_freqItems|\n",
      "+--------------------+-----------------------+\n",
      "|[146, 137, 101, 1...|   [92, 101, 83, 95,...|\n",
      "+--------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util.stat.freqItems(['server_id','session_count']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1fd364-7a0b-450e-a6e1-e26e5ae4b367",
   "metadata": {},
   "source": [
    "We can create a result-set that shows some basic statistics for one of the columns by using Spark SQL. Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9a9b3cb-ea23-4ccb-977d-3486e9fd64d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------------+\n",
      "|min(cpu_utilization)|max(cpu_utilization)|stddev(cpu_utilization)|\n",
      "+--------------------+--------------------+-----------------------+\n",
      "|                0.22|                 1.0|     0.1587517387291281|\n",
      "+--------------------+--------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT min(cpu_utilization),\\\n",
    "            max(cpu_utilization),stddev(cpu_utilization) FROM utilization\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4694eb25-71f5-40c1-887e-66575835eebf",
   "metadata": {},
   "source": [
    "And if we want to group the result-set by `server_id`, we can write the following query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96460c10-5bf7-4475-9bdc-8d0e9e4c02e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+-----------------------+\n",
      "|server_id|min(cpu_utilization)|max(cpu_utilization)|stddev(cpu_utilization)|\n",
      "+---------+--------------------+--------------------+-----------------------+\n",
      "|      148|                0.54|                0.94|    0.11451712518744131|\n",
      "|      137|                0.54|                0.94|    0.11526245077758812|\n",
      "|      133|                0.55|                0.95|    0.11534006553263144|\n",
      "|      108|                0.55|                0.95|    0.11563100171171926|\n",
      "|      101|                 0.6|                 1.0|    0.11651726263197697|\n",
      "|      115|                0.44|                0.84|    0.11569664615014985|\n",
      "|      126|                0.48|                0.88|    0.11542612970702051|\n",
      "|      103|                0.56|                0.96|    0.11617507884178278|\n",
      "|      128|                0.38|                0.78|     0.1153254132405078|\n",
      "|      122|                0.43|                0.83|    0.11563104329209034|\n",
      "|      111|                0.36|                0.76|    0.11530221569464506|\n",
      "|      140|                0.47|                0.87|    0.11539940805020545|\n",
      "|      132|                0.33|                0.73|     0.1145442656350766|\n",
      "|      146|                 0.5|                 0.9|    0.11488129439634706|\n",
      "|      142|                 0.5|                 0.9|    0.11593003726970044|\n",
      "|      139|                0.51|                0.91|    0.11519660023052102|\n",
      "|      120|                0.35|                0.75|    0.11586355920838642|\n",
      "|      117|                0.38|                0.78|    0.11534593941519553|\n",
      "|      112|                0.52|                0.92|    0.11528867845082576|\n",
      "|      127|                0.47|                0.87|    0.11577746913037888|\n",
      "+---------+--------------------+--------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT server_id, min(cpu_utilization),max(cpu_utilization),stddev(cpu_utilization) \\\n",
    "           FROM utilization \\\n",
    "           GROUP BY server_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8d1e5a-1774-4aec-bd08-379ad1a81e2c",
   "metadata": {},
   "source": [
    "Now, we are going to calculate statistics on buckets or histograms of data. The idea is, rather than look at each server individually, Spark buckets values according to how frequently they occur in certain ranges. So if we want to know how often does a CPU utilization fall in the range of 1-10 or 11-20 or 21-30, all the way up to 90-91, we could put each of those CPU utilization measures into its bucket and count how many times a CPU utilization goes into that bucket. Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "629b3e31-90f1-4485-afe7-7a41bc917fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|server_id|Bucket|\n",
      "+---------+------+\n",
      "|      100|     5|\n",
      "|      100|     4|\n",
      "|      100|     5|\n",
      "|      100|     5|\n",
      "|      100|     3|\n",
      "|      100|     4|\n",
      "|      100|     5|\n",
      "|      100|     4|\n",
      "|      100|     5|\n",
      "|      100|     5|\n",
      "|      100|     3|\n",
      "|      100|     6|\n",
      "|      100|     6|\n",
      "|      100|     5|\n",
      "|      100|     2|\n",
      "|      100|     4|\n",
      "|      100|     4|\n",
      "|      100|     6|\n",
      "|      100|     4|\n",
      "|      100|     5|\n",
      "+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT server_id, FLOOR(cpu_utilization*100/10) as Bucket FROM utilization\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1629d8b3-0e13-49fd-8af7-77ea786d554b",
   "metadata": {},
   "source": [
    "So far, what we have done is we have listed for each server in what  CPU utilization bucket falls at a particular time. Now we want to see how often does a CPU utilization falls into one of those ten buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5acea851-ebdd-41a3-82ba-36ba220c838d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|count(1)|Bucket|\n",
      "+--------+------+\n",
      "|    8186|     2|\n",
      "|   37029|     3|\n",
      "|   68046|     4|\n",
      "|  104910|     5|\n",
      "|  116725|     6|\n",
      "|   88242|     7|\n",
      "|   56598|     8|\n",
      "|   20207|     9|\n",
      "|      57|    10|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*), FLOOR(cpu_utilization*100/10) as Bucket \\\n",
    "           FROM utilization GROUP BY Bucket ORDER BY Bucket\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e790cf28-0e21-474a-9542-bc5606588771",
   "metadata": {},
   "source": [
    "## Timeseries Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca186ce-e9cc-4089-b9f8-a722fd521fa1",
   "metadata": {},
   "source": [
    "In this section, we are going to work with timeseries data, and timeseries data has a set of measures and a timestamp associated with them. First, let's take a look at utilization table again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b7b87e3a-95ee-4a0f-a502-06023024bfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+-----------------------+\n",
      "|server_id|min(cpu_utilization)|max(cpu_utilization)|stddev(cpu_utilization)|\n",
      "+---------+--------------------+--------------------+-----------------------+\n",
      "|      148|                0.54|                0.94|    0.11451712518744131|\n",
      "|      137|                0.54|                0.94|    0.11526245077758812|\n",
      "|      133|                0.55|                0.95|    0.11534006553263144|\n",
      "|      108|                0.55|                0.95|    0.11563100171171926|\n",
      "|      101|                 0.6|                 1.0|    0.11651726263197697|\n",
      "|      115|                0.44|                0.84|    0.11569664615014985|\n",
      "|      126|                0.48|                0.88|    0.11542612970702051|\n",
      "|      103|                0.56|                0.96|    0.11617507884178278|\n",
      "|      128|                0.38|                0.78|     0.1153254132405078|\n",
      "|      122|                0.43|                0.83|    0.11563104329209034|\n",
      "|      111|                0.36|                0.76|    0.11530221569464506|\n",
      "|      140|                0.47|                0.87|    0.11539940805020545|\n",
      "|      132|                0.33|                0.73|     0.1145442656350766|\n",
      "|      146|                 0.5|                 0.9|    0.11488129439634706|\n",
      "|      142|                 0.5|                 0.9|    0.11593003726970044|\n",
      "|      139|                0.51|                0.91|    0.11519660023052102|\n",
      "|      120|                0.35|                0.75|    0.11586355920838642|\n",
      "|      117|                0.38|                0.78|    0.11534593941519553|\n",
      "|      112|                0.52|                0.92|    0.11528867845082576|\n",
      "|      127|                0.47|                0.87|    0.11577746913037888|\n",
      "+---------+--------------------+--------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT server_id, min(cpu_utilization), max(cpu_utilization), stddev(cpu_utilization) \\\n",
    "           FROM utilization \\\n",
    "           GROUP BY server_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b085af44-8512-4406-9754-e80a0bd80415",
   "metadata": {},
   "source": [
    "Sometimes we might want to compare a value within a group. For example, we would like to compare the current CPU utilization for a server to the average CPU utilization of just that server, not the entire population.\n",
    "\n",
    "We can do that using a windowing function, and in SQL, the windowing functions are specified using an `OVER...PARTITION BY` statement. So let's take a look at how to use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3eb1da2b-48cb-4c48-8a7b-6d2957aeac96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------------+\n",
      "|     event_datetime|server_id|cpu_utilization|  avg_server_util|\n",
      "+-------------------+---------+---------------+-----------------+\n",
      "|03/05/2019 08:06:14|      100|           0.57|0.467506000000003|\n",
      "|03/05/2019 08:11:14|      100|           0.47|0.467506000000003|\n",
      "|03/05/2019 08:16:14|      100|           0.56|0.467506000000003|\n",
      "|03/05/2019 08:21:14|      100|           0.57|0.467506000000003|\n",
      "|03/05/2019 08:26:14|      100|           0.35|0.467506000000003|\n",
      "|03/05/2019 08:31:14|      100|           0.41|0.467506000000003|\n",
      "|03/05/2019 08:36:14|      100|           0.57|0.467506000000003|\n",
      "|03/05/2019 08:41:14|      100|           0.41|0.467506000000003|\n",
      "|03/05/2019 08:46:14|      100|           0.53|0.467506000000003|\n",
      "|03/05/2019 08:51:14|      100|           0.51|0.467506000000003|\n",
      "|03/05/2019 08:56:14|      100|           0.32|0.467506000000003|\n",
      "|03/05/2019 09:01:14|      100|           0.62|0.467506000000003|\n",
      "|03/05/2019 09:06:14|      100|           0.66|0.467506000000003|\n",
      "|03/05/2019 09:11:14|      100|           0.54|0.467506000000003|\n",
      "|03/05/2019 09:16:14|      100|           0.29|0.467506000000003|\n",
      "|03/05/2019 09:21:14|      100|           0.43|0.467506000000003|\n",
      "|03/05/2019 09:26:14|      100|           0.49|0.467506000000003|\n",
      "|03/05/2019 09:31:14|      100|           0.64|0.467506000000003|\n",
      "|03/05/2019 09:36:14|      100|           0.42|0.467506000000003|\n",
      "|03/05/2019 09:41:14|      100|           0.55|0.467506000000003|\n",
      "+-------------------+---------+---------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT event_datetime, server_id, cpu_utilization, \\\n",
    "           avg(cpu_utilization) OVER (PARTITION BY server_id) as avg_server_util \\\n",
    "           FROM Utilization').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf970424-95fb-42b9-b3b2-35609645f311",
   "metadata": {},
   "source": [
    "Now, we have different timestamps for each server ID, different CPU utilization at those particular times, but in this piece of result-set, the average server utilization is always 0.7153 for server ID 112.\n",
    "\n",
    "Now, we want to calculate the difference any one of these measurements of CPU utilization from the average of that server is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f9c0117-4ac5-41b4-98d3-f1fab43462a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------------+--------------------+\n",
      "|     event_datetime|server_id|cpu_utilization|  avg_server_util|   delta_server_util|\n",
      "+-------------------+---------+---------------+-----------------+--------------------+\n",
      "|03/05/2019 08:06:14|      100|           0.57|0.467506000000003| 0.10249399999999698|\n",
      "|03/05/2019 08:11:14|      100|           0.47|0.467506000000003|0.002493999999996...|\n",
      "|03/05/2019 08:16:14|      100|           0.56|0.467506000000003| 0.09249399999999708|\n",
      "|03/05/2019 08:21:14|      100|           0.57|0.467506000000003| 0.10249399999999698|\n",
      "|03/05/2019 08:26:14|      100|           0.35|0.467506000000003|  -0.117506000000003|\n",
      "|03/05/2019 08:31:14|      100|           0.41|0.467506000000003|  -0.057506000000003|\n",
      "|03/05/2019 08:36:14|      100|           0.57|0.467506000000003| 0.10249399999999698|\n",
      "|03/05/2019 08:41:14|      100|           0.41|0.467506000000003|  -0.057506000000003|\n",
      "|03/05/2019 08:46:14|      100|           0.53|0.467506000000003| 0.06249399999999705|\n",
      "|03/05/2019 08:51:14|      100|           0.51|0.467506000000003|0.042493999999997034|\n",
      "|03/05/2019 08:56:14|      100|           0.32|0.467506000000003|-0.14750600000000297|\n",
      "|03/05/2019 09:01:14|      100|           0.62|0.467506000000003| 0.15249399999999702|\n",
      "|03/05/2019 09:06:14|      100|           0.66|0.467506000000003| 0.19249399999999706|\n",
      "|03/05/2019 09:11:14|      100|           0.54|0.467506000000003| 0.07249399999999706|\n",
      "|03/05/2019 09:16:14|      100|           0.29|0.467506000000003|  -0.177506000000003|\n",
      "|03/05/2019 09:21:14|      100|           0.43|0.467506000000003|-0.03750600000000298|\n",
      "|03/05/2019 09:26:14|      100|           0.49|0.467506000000003|0.022493999999997016|\n",
      "|03/05/2019 09:31:14|      100|           0.64|0.467506000000003| 0.17249399999999704|\n",
      "|03/05/2019 09:36:14|      100|           0.42|0.467506000000003|-0.04750600000000299|\n",
      "|03/05/2019 09:41:14|      100|           0.55|0.467506000000003| 0.08249399999999707|\n",
      "+-------------------+---------+---------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT event_datetime, server_id, cpu_utilization, \\\n",
    "           avg(cpu_utilization) OVER (PARTITION BY server_id) as avg_server_util, \\\n",
    "           cpu_utilization - avg(cpu_utilization) OVER (PARTITION BY server_id) as delta_server_util \\\n",
    "           FROM Utilization').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b2e6b6-f37f-436e-ab20-3f3845772681",
   "metadata": {},
   "source": [
    "That is one of the operations that we can do with the windowing functions. We can compare a particular value in a row to a value of some aggregate function applied to a sub-set of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dcaab4-038e-486e-ac04-5b8191df0af5",
   "metadata": {},
   "source": [
    "Another operation that we can do with windowing functions is looking around the neighborhood of a row. For example, we might want to calculate in a sliding window, look at the last three values and average them or look at the previous value, current value, next value, and average them. Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "60a0affe-fce8-46fd-89ad-396a1bc2df94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-------------------+\n",
      "|     event_datetime|server_id|cpu_utilization|    avg_server_util|\n",
      "+-------------------+---------+---------------+-------------------+\n",
      "|03/05/2019 08:06:14|      100|           0.57|               0.52|\n",
      "|03/05/2019 08:11:14|      100|           0.47| 0.5333333333333333|\n",
      "|03/05/2019 08:16:14|      100|           0.56| 0.5333333333333333|\n",
      "|03/05/2019 08:21:14|      100|           0.57|0.49333333333333335|\n",
      "|03/05/2019 08:26:14|      100|           0.35| 0.4433333333333333|\n",
      "|03/05/2019 08:31:14|      100|           0.41|0.44333333333333336|\n",
      "|03/05/2019 08:36:14|      100|           0.57| 0.4633333333333333|\n",
      "|03/05/2019 08:41:14|      100|           0.41| 0.5033333333333333|\n",
      "|03/05/2019 08:46:14|      100|           0.53|0.48333333333333334|\n",
      "|03/05/2019 08:51:14|      100|           0.51|0.45333333333333337|\n",
      "|03/05/2019 08:56:14|      100|           0.32| 0.4833333333333334|\n",
      "|03/05/2019 09:01:14|      100|           0.62| 0.5333333333333333|\n",
      "|03/05/2019 09:06:14|      100|           0.66| 0.6066666666666667|\n",
      "|03/05/2019 09:11:14|      100|           0.54|0.49666666666666676|\n",
      "|03/05/2019 09:16:14|      100|           0.29|               0.42|\n",
      "|03/05/2019 09:21:14|      100|           0.43| 0.4033333333333333|\n",
      "|03/05/2019 09:26:14|      100|           0.49|               0.52|\n",
      "|03/05/2019 09:31:14|      100|           0.64| 0.5166666666666666|\n",
      "|03/05/2019 09:36:14|      100|           0.42| 0.5366666666666667|\n",
      "|03/05/2019 09:41:14|      100|           0.55|               0.42|\n",
      "+-------------------+---------+---------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT event_datetime, server_id, cpu_utilization, \\\n",
    "           avg(cpu_utilization) OVER (PARTITION BY server_id ORDER BY event_datetime \\\n",
    "                                       ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) as avg_server_util \\\n",
    "           FROM Utilization').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb16a4c-5eef-449f-887f-cdaccaad7b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
